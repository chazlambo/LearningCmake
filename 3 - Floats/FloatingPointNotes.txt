Floating Point Numbers:
	Store real numbers, eg. 10.524
	Limited precision and accuracy
	Stored similar to scientific notation, eg. 1.01010*2^129
		Float: Accurate to ~6-7 decimal digits
		Double: Accurate to ~16-17 decimal digits
	Behaviour specified by IEEE-754 standard (rounding, size, range, etc)

Floating Point vs Fixed Point
	Consider a "toy" number system with 8 bits to represent real numbers
	NOTE: ALL NUMBERS ARE IN BINARY UNLESS OBVIOUSLY IN DECIMAL

	Fixed Point: 1 bit for sign, 3 bits before the decimal point
		Smallest Value:
			-111.1111 = -7.9375
		Largest Value:
			111.1111 = 7.9375
		Difference between 1 and next number
			001.0001 - 001.0000 = 000.0001
			==> 0.0625
		Difference between 4 and next number
			100.0001 - 100.0000 = 000.0001
			==> 0.0625
		Precision is effectively the same

	Floating Point: 1 bit for sign, 3 bits for exponent, 4 bits for mantissa
		Smallest Value:
			-1.111 * 2^(111b) = -(1 + 1/2 + 1/4 + 1/8) * 2^7 = -240
		Largest Value
			1.111 * 2^(111b) = -(1 + 1/2 + 1/4 + 1/8) * 2^7 = 240
		Difference between 1 and next number
			1.001 * 2^0 - 1.000 * 2^0 = 0.001
			==> 0.0625
		Difference between 4 and next number
			1.001 * 2^2 - 1.000 * 2^2 = 0.100
			==> 0.5
		Differences between values are not uniform
		Numbers are not distributed evenly
			As numbers get larger, precision gets coarser

Floats in C+
	Normalized scientific notation, in Base-2:
		(-1)^S * M * 2^E
		S = Sign, E = Exponent, M = Mantissa/Fraction
		Mantissa is fixed to start with a 1
		e.g. 0.1 = (-1)^0 * 1.6 * 2^(-4)

	32-bit Single Precision 1 bit sign, 8-bit exponent, 23-bit mantissa
		Accuracy: log10(2^24) + 7 significant digits
		Range: -3.4E+38 to 3.4E+38
		Example Number:
			0.1 = (-1)^0 * 1.6 * 2^(-4)
			0.1 = 0011 1101 1100 1100 1100 1100 1100 1100 1101

